# -*- coding: utf-8 -*-
"""models_diabetes.ipnyb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fGLo5f83oGIJ0EqMDGzUKZCXG-7ZfQu-

Martí, Miquel and Pere. June 2022.

This are the instructions we have used to build the models.
"""

import sys
!{sys.executable} -m pip install pandas
!{sys.executable} -m pip install numpy
!{sys.executable} -m pip install matplotlip
!{sys.executable} -m pip install math
!{sys.executable} -m pip install scipy
!{sys.executable} -m pip install scikit-learn
!{sys.executable} -m pip install easyinput
!{sys.executable} -m pip install seaborn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
from scipy.io import arff
from scipy import stats
from sklearn.impute import KNNImputer
from easyinput import read

import seaborn as sns
from numpy import isnan
from pandas import read_csv

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.impute import KNNImputer
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

def ask(title):
    print(title + "? [y/n]", end = " ")
    qt = read(str)
    if (qt == "y"):
        print("\n")
        return True
    else:
        return False

# PERFORM THE PREVIOUSLY JUSTIFIED PREPROCESSING

db_data = pd.read_csv('diabetes.csv', sep=',')

db_mod = db_data

for i in range(0, 768):
    if (db_mod.loc[i, 'class'] == 'tested_positive'):
        db_mod.loc[i, 'class'] = 1
    elif (db_mod.loc[i, 'class'] == 'tested_negative'):
        db_mod.loc[i, 'class'] = 0

db_mod = db_mod.drop('id', axis=1)

db_mod[['plas', 'pres', 'skin', 'insu', 'mass']] = db_mod[[
    'plas', 'pres', 'skin', 'insu', 'mass']].replace(0, np.NaN)

imputer = KNNImputer()
imputed = imputer.fit_transform(db_mod)
db_mod = pd.DataFrame(imputed, columns=db_mod.columns)

db_mod['log_insu'] = db_mod['insu']
db_mod['log_insu'] = np.log(db_mod['insu'])
db_mod = db_mod.apply(lambda iterator: ((iterator.max() - iterator) /
                                        (iterator.max() - iterator.min())).round(2))

# RESAMPLING
# 75% train, 22% validation, 3% test

train, remaining_data = np.split(db_mod,
                                   [int(0.75 * len(db_mod))])
validation, test = np.split(remaining_data,[int(0.885 * len(remaining_data))])

x_train = train.drop(["class"],axis=1)
y_train = train["class"]
x_val = validation.drop(["class"],axis=1)
y_val = validation["class"]
x_test = test.drop(["class"],axis=1)
y_test = test["class"]
print(x_train.describe())
print(x_val.describe())
print(x_test.describe())

## LOGISTIC REGRESSION MODEL
# ELAPSED TIME: approx 20 seconds

if (ask("LOGISTIC REGRESSION")):
    model = LogisticRegression()

    # Configuration of the KFold CV
    solvers = ['newton-cg', 'lbfgs', 'liblinear']
    penalty = ['l2']
    c_values = [100, 10, 1.0, 0.1, 0.01]
    grid = dict(solver=solvers, penalty=penalty, C=c_values)

    # Perform cross validation
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)
    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-
                               1, cv=cv, scoring='accuracy', error_score=0)
    grid_result_logi = grid_search.fit(x_train, y_train)

    try:
      del(zip)
    except:
      print('')

    # Summarise results LR
    means = grid_result_logi.cv_results_['mean_test_score']
    stds = grid_result_logi.cv_results_['std_test_score']
    params = grid_result_logi.cv_results_['params']
    zip = sorted(zip(means, stds,params), key = lambda x: -x[0]) # Sort them by accuracy desc.
    for mean, stdev, param in zip[0:5]: # Print first five
        print("%f %f with: %r" % (mean, stdev, param))

## SECOND ROUND LR -> search around C=10
if (ask("SECOND LOGISTIC REGRESSION")):
    model = LogisticRegression()

    # Configuration of the KFold CV 
    solvers = ['newton-cg', 'lbfgs', 'liblinear']
    penalty = ['l2']
    c_values = [5, 7, 10, 12, 15]
    grid = dict(solver=solvers, penalty=penalty, C=c_values)

     # Perform cross validation
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)
    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-
                               1, cv=cv, scoring='accuracy', error_score=0)
    grid_result_logi = grid_search.fit(x_train, y_train)

    try:
      del(zip)
    except:
      print('')
    # Summarise results LR
    means = grid_result_logi.cv_results_['mean_test_score']
    stds = grid_result_logi.cv_results_['std_test_score']
    params = grid_result_logi.cv_results_['params']
    zip = sorted(zip(means, stds,params), key = lambda x: -x[0]) # Sort them by accuracy desc.
    for mean, stdev, param in zip[0:5]: # Print first five
        print("%f %f with: %r" % (mean, stdev, param))

        ## NO IMPROVES IN THE SECOND SEARCH. LOGISTIC REGRERSSION IS TUNED AT ITS PEAK.

## SUPPORT VECTOR CLASSIFIER MODEL
# ELAPSED TIME: approx 1m 48seg
if (ask("SUPPORT VECTOR CLASSIFIER")):
    model = SVC()

    # Configuration of the KFold CV 
    param_grid = {'C': [0.1, 1, 10, 100, 1000],
                  'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
                  'kernel': ["linear",'rbf']}

    # Perform cross validation
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-
                               1, cv=cv, scoring='accuracy', error_score=0)
    grid_result_svc = grid_search.fit(x_train, y_train)

    try:
      del(zip)
    except:
      print('')

    # Summarise results SVC
    means = grid_result_svc.cv_results_['mean_test_score']
    stds = grid_result_svc.cv_results_['std_test_score']
    params = grid_result_svc.cv_results_['params']
    zip = sorted(zip(means, stds,params), key = lambda x: -x[0]) # Sort them by accuracy desc.

    # Print first five
    for mean, stdev, param in zip[0:5]:
        print("%f %f with: %r" % (mean, stdev, param))

    # Print all results with Gaussian kernel. Reasons explained on the main repport. 
    for mean, stdev, param in zip:
        if (param['kernel'] == 'rbf'):
          print("%f %f with: %r" % (mean, stdev, param))

## SECOND ROUND SVC -> search around C = 1 with linear kernel.

if (ask("SECOND SUPPORT VECTOR CLASSIFIER")):
    model = SVC()
    
    # Configuration of the KFold CV
    param_grid = {'C': [0.5, 0.6, 0.7, 0.8, 0.9, 1],
                  'kernel': ["linear",]}

    # Perform cross validation
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-
                               1, cv=cv, scoring='accuracy', error_score=0)
    grid_result_svc = grid_search.fit(x_train, y_train)

    try:
      del(zip)
    except:
      print('')

    # Summarise results SVC
    means = grid_result_svc.cv_results_['mean_test_score']
    stds = grid_result_svc.cv_results_['std_test_score']
    params = grid_result_svc.cv_results_['params']
    zip = sorted(zip(means, stds,params), key = lambda x: -x[0]) # Sort them by accuracy desc.

    # Print first five
    for mean, stdev, param in zip[0:5]:
        print("%f %f with: %r" % (mean, stdev, param))

## RANDOM FOREST
# ELAPSED TIME: approx 30 minutes

if (ask("RANDOM FOREST")):
    model = RandomForestClassifier(random_state=42)
    
     # Configuration of the KFold CV
    param_grid = {
        'n_estimators': [200,500],
        'max_features': ['auto', 'sqrt', 'log2'],
        'max_depth' : [4,8],
        'criterion' :['gini', 'entropy']
    }

    # Perform cross validation
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)
    grid_search = GridSearchCV(estimator = model, param_grid = param_grid, n_jobs =-1, cv = cv, scoring = 'accuracy', error_score = 0)
    grid_result_rf = grid_search.fit(x_train, y_train)

    try:
      del(zip)
    except:
      print('')

    # Summarise results Random Forest
    means = grid_result_rf.cv_results_['mean_test_score']
    stds = grid_result_rf.cv_results_['std_test_score']
    params = grid_result_rf.cv_results_['params']
    zip = sorted(zip(means, stds,params), key = lambda x: -x[0]) # Sort them by accuracy asc. 

    # Print first five
    for mean, stdev, param in zip[0:5]:
        print("%f %f with: %r" % (mean, stdev, param))

# SECOND ROUND RANDOM FOREST

if (ask("RANDOM FOREST")):
    model = RandomForestClassifier(random_state=42)
    
     # Configuration of the KFold CV
    param_grid = {
        'n_estimators': [300,400,600,700],
        'max_features': ['auto'],
        'max_depth' : [3,4,5],
        'criterion' :['gini']
    }

    # Perform cross validation
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)
    grid_search = GridSearchCV(estimator = model, param_grid = param_grid, n_jobs =-1, cv = cv, scoring = 'accuracy', error_score = 0)
    grid_result_rf = grid_search.fit(x_train, y_train)

    try:
      del(zip)
    except:
      print('')

    # Summarise results Random Forest
    means = grid_result_rf.cv_results_['mean_test_score']
    stds = grid_result_rf.cv_results_['std_test_score']
    params = grid_result_rf.cv_results_['params']
    zip = sorted(zip(means, stds,params), key = lambda x: -x[0]) # Sort them by accuraci asc. 

    # Print first five
    for mean, stdev, param in zip[0:5]:
        print("%f %f with: %r" % (mean, stdev, param))

## NEAREST NEIGHBOURS CLASSIFIER
# ELAPSED TIME: approx 6 minutes

if (ask("NEAREST NEIGHBOURS CLASSIFIER")):
    model = KNeighborsClassifier()
    
    # Configuration of the KFold CV
    param_grid = {
        'leaf_size': [1, 20, 40], # leaf value
        'n_neighbors': [30, 40, 50, 60, 70], #number of neighbours used
        'p' : [1, 2, 3, 4],
        'weights' : ['uniform', 'distance']
    }

    # Perform cross validation
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)
    grid_search = GridSearchCV(estimator = model, param_grid = param_grid, n_jobs =-1, cv = cv, scoring = 'accuracy', error_score = 0)
    grid_result_knn = grid_search.fit(x_train, y_train)

    try:
      del(zip)
    except:
      print('')
    # Summarise results KNN
    means = grid_result_knn.cv_results_['mean_test_score']
    stds = grid_result_knn.cv_results_['std_test_score']
    params = grid_result_knn.cv_results_['params']
    zip = sorted(zip(means,stds,params), key = lambda x: -x[0]) # sort by accuracy asc.

    # Print first five
    for mean, stdev, param in zip[0:5]:
        print("%f %f with: %r" % (mean, stdev, param))

if (ask("SECOND NEAREST NEIGHBOURS CLASSIFIER")):
    model = KNeighborsClassifier()
    
    # Configuration of the KFold CV
    param_grid = {
        'leaf_size': [20], # leaf value
        'n_neighbors': [60, 62, 64, 66, 68, 70], #number of neighbours used
        'p' : [1],
        'weights' : ['uniform']
    }

    # Perform cross validation
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)
    grid_search = GridSearchCV(estimator = model, param_grid = param_grid, n_jobs =-1, cv = cv, scoring = 'accuracy', error_score = 0)
    grid_result_knn = grid_search.fit(x_train, y_train)

    try:
      del(zip)
    except:
      print('')

    # Summarise results KNN
    means = grid_result_knn.cv_results_['mean_test_score']
    stds = grid_result_knn.cv_results_['std_test_score']
    params = grid_result_knn.cv_results_['params']
    zip = sorted(zip(means,stds,params), key = lambda x: -x[0]) # sort by accuracy asc.

    # Print first five
    for mean, stdev, param in zip[0:5]:
        print("%f %f with: %r" % (mean, stdev, param))

### SELECTION

# TRAIN MODEL WITH BEST PARAMETERS
# LOGISTIC REGRESSION
model1 = LogisticRegression(C=10, solver='liblinear')
model_log_reg = model1.fit(x_train, y_train)
y_true, y_pred = y_val, model_log_reg.predict(x_val)
accuracy = accuracy_score(y_true, y_pred)
print("Detailed classification report:")
print("Accuracy : ", accuracy)
print(classification_report(y_true, y_pred))

# SVC
model2 = SVC(C=0.7, kernel='linear')
model_svc = model2.fit(x_train, y_train)
y_true, y_pred = y_val, model_svc.predict(x_val)
accuracy = accuracy_score(y_true, y_pred)
print("Detailed classification report:")
print("Accuracy : ", accuracy)
print(classification_report(y_true, y_pred))

# RANDOM FOREST CLASSIFIER
model3 = RandomForestClassifier(n_estimators=300, criterion='gini', max_depth=3,max_features='log2')
model_rf = model3.fit(x_train, y_train)
y_true, y_pred = y_val, model_rf.predict(x_val)
accuracy = accuracy_score(y_true, y_pred)
print("Detailed classification report:")
print("Accuracy : ", accuracy)
print(classification_report(y_true, y_pred))

# RANDOM FOREST CLASSIFIER
model4 = KNeighborsClassifier(n_neighbors=62, weights='uniform', leaf_size=20, p=1)
model_knn = model4.fit(x_train, y_train)
y_true, y_pred = y_val, model_knn.predict(x_val)
accuracy = accuracy_score(y_true, y_pred)
print("Detailed classification report:")
print("Accuracy : ", accuracy)
print(classification_report(y_true, y_pred))

y_true, y_pred = y_val, model_log_reg.predict(x_val)
std_log_reg = mean_squared_error(y_true, y_pred)
print("Standard error: ", std_log_reg)
y_true, y_pred = y_val, model_svc.predict(x_val)
std_svc = mean_squared_error(y_true, y_pred)
print("Standard error: ", std_svc)

# PREDICTIONS
x_refit = pd.concat([x_train, x_val])
y_refit = pd.concat([y_train, y_val])

model_test= SVC(C=0.7, kernel='linear')
model_svc_test = model_test.fit(x_refit, y_refit)
y_true, y_pred = y_test, model_svc.predict(x_test)

print("REAL, PRED SVC, CLASSIFIED")
y_true = np.array(y_true_svc)
bad_classified = 0
for i in range(23):
  if(y_true[i] == y_pred[i] ):
    classified="YES"
  else:
    classified = "NO"
    bad_classified += 1
  print( y_true[i], y_pred[i], classified)
print("Instances not classified correctly:",bad_classified)

accuracy = accuracy_score(y_true, y_pred)
print("Accuracy:", accuracy)
print("Generalization error:", (1-accuracy)*100)